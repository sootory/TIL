# 자습
## 딥러닝 이론 정리
1. 오차역전파법(backpropagation)
- 수치 미분은 계산 시간이 오래걸린다는 단점이 있는 반면, 오차역전파법은 가중치 매개변수의 기울기를 효율적으로 계산하는 방법임. 이후 수치 미분은 오차역전파법으로 구한 기울기 값을 검증하는 수단으로 활용할 수 있음
- 계산 그래프의 역전파: 순방향과는 반대 방향(오른쪽에서 왼쪽으로 전달)으로 국소적 미분을 곱함
- 덧셈 노드의 역전파는 입력 값을 그대로 보냄
- 곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보냄(순전파 때 받았던 값이 x라면 y를, 순전파 때 받았던 값이 y라면 x를 앞에서 전달받은 값에 곱해서 다음으로 보낸다는 의미)

2. 학습 관련 기술들
- 최적의 매개변수를 찾는 방법(optimizer)으로 확률적경사하강법(SGD) 외에 모멘텀, AdaGrad, Adam이 있음
- 모멘텀은 공이 그릇의 바닥을 구르는 듯한 움직임을 보여주며 지그재그의 움직임이 SGD보다 덜함
- AdaGrad는 각각의 매개변수에 맞는 값을 만들어주는 방법으로 즉, 개별 매개변수에 적응적으로(adaptive) 학습률을 조정하면서 학습을 진행함(매개변수의 원소 중에 크게 갱신된 원소는 학습률이 낮아짐)
- Adam은 모멘텀과 AdaGrad를 융합한 기법이라고 볼 수 있음
- 위의 네 가지 최적화 방법 중 어느 것이 무조건 뛰어나다고 보기는 어려움. 여러 가지로 시도해 보는 것을 추천

참고 자료: [도서] 밑바닥부터 시작하는 딥러닝