# 세미프로젝트
## 자연어처리(NLP) 이론 심화_4장 단어 수준 임베딩(1)
1. NPLM
 - 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습으로 직전까지 등장한 단어들로 다음 단어를 맞춤
 - NPLM은 새로운 데이터여도 문맥이 비슷한 다른 문장을 참고해서 확률을 부여하기 때문에 그 자체로 언어 모델 역할을 수행할 수 있음

2. Word2Vec_skip-gram 모델 위주
 - 타깃 단어와 문맥 단어의 쌍을 포지티브 샘플, 타깃과 주변에 등장하지 않은 단어의 쌍인 네거티브 샘플이라고 함
 - 해당 모델은 다켓 단어와 문맥 단어 쌍이 주어졌을 때 포지티브 샘플인지 아닌지를 예측하는 과정에서 학습됨. 포지티브는 포지티브로 네거티브는 네거티브로 맞춰야 함
 - 모델을 실행하면 기준 단어와 코사인 유사도가 높은 단어를 출력
  
3. FastText
 - 네거티브 샘플링 기법을 쓰며 Word2Vec과의 차이점은 각 단어를 문자 단위 n-gram으로 표현한다는 점임
 - 한글과 궁합이 잘 맞는 편이며, 오타나 미등록 단어에도 강건함(robust)

4. 잠재 의미 분석
 - 잠재 의미 분석은 데이터의 차원 수를 줄여 계산 효율성을 높이는 동시에 행간에 숨어 있는 의미를 이끌어 내기 위한 방법론
 - 자연어 처리 분야에서는 PPMI(positive pointwise mutual information)이란 지표를 사용하는데 이는 두 확률 변수의 상관성을 계량화한 지표가 양수가 아닐 경우 0으로 치환해 무시하는 것을 의미함
 - 단어-문서 행렬, TF-IDF 행렬, 단어-문맥 행렬, 점별 상호 정보량에 모두 잠재 의미 분석을 수행할 수 있음


참고 자료: [도서] 한국어 임베딩, 딥러닝을 이용한 자연어 처리 입문   