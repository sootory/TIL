# 세미프로젝트
## 자연어처리(NLP) 이론 심화_서론
1. 임베딩이란?
   - 자연어를 기계가 이해할 수 있게 벡터로 바꾼 것 또는 바꾸는 과정 전체를 의미
2. 임베딩의 역할
   - 단어/문장 간 관련도 계산
     - 단어를 벡터화하면 단어 벡터들 사이의 유사도를 계산할 수 있음(코사인 유사도 활용 가능)
   - 의미/문법 정보 함축
     - 벡터는 사칙연산이 가능하기 때문에 덧셈이나 뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출할 수 있음
     - 첫번째단어벡터-두번째단어벡터 + 세번째 단어벡터 계산
     - 이를 단어 유추 평가라 부름
   - 전이학습
     - 임베딩한 값을 다른 딥러닝의 입력으로 쓰는 것
     - 전이학습 모델을 통해 문서 분류를 더 효율적으로 수행할 수 있음
3. 임베딩 기법의 역사와 종류
   - 통계 기반에서 뉴럴 네트워크 기반으로
     - 잠재 의미 분석: 수학적 기법을 활용해 벡터의 차원을 축소하는 기법으로 TF-IDF 행렬, 단어-문맥 행렬, 점별 상호 정보랼 행렬 등을 분석 대상으로 삼음
     - 뉴럴 네트워크 기반은 다음 단어를 예측하거나 가려진 단어가 무엇일지 맞추며 학습 진행
   - 단어 수준에서 문장 수준으로
     - 2017년 이전은 NPLM, Word2Vec, FastText와 같은 단어 수준 모델이 주였음. 하지만, 단어 임베딩 기법은 동음이의어를 분간하기 어려움
     - 이후 BERT나 GPT와 같은 문장 수준 임베딩 기법이 주목 받음. 단어 시쿼스 전체의 문맥적 의미를 함축하기 때문에 전이 학습 효과가 좋음
   - 룰 → 엔드투엔드 → 프리트레인/파인 튜닝
     - 90년대까지는 사람이 feature를 직접 뽑음. 문법적 규칙들을 모델에 알려줌
     - 00년대 중반 이후 딥러닝 모델에 주목하며 컴퓨터 스스로 입출력 사이의 관계를 이해하도록 유도함. sequence to sequence 모델이 대표적
     - 18년 ELmo 모델 이후 pretrain과 fine tuning의 방식으로 발전 중. pretrain은 대규모 말뭉치로 임베딩을 만드는 것임. fine tuning은 해당 임베딩을 input으로 하여 딥러닝 모델을 만들고 새로운 데이터에 맞게 임베딩을 포함한 전체 모델을 업데이트하는 것을 의미함
    - 임베딩의 종류와 성능
      - 행렬 분해: 원래 행렬을 작은 행렬로 쪼개는 기법. GloVe, Swivel 등
      - 예측 기반: 어떤 단어 주변에 나타날 특정 단어를 예측하거나 다음 단어 예측, 지워진 단어를 맞추는 과정에서 학습하는 방법. Word2Vec, FastText, BERT, ElMo 등
      - 토픽 기반: 잠재된 주제를 추론하는 방식으로 LDA가 대표적임. LDA는 학습이 완료되면 각 문서의 주제 분포를 확률 벡터로 반환함

참고 자료: [도서] 한국어 임베딩