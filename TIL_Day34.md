# 세미프로젝트
## 자연어처리(NLP) 이론 심화_4장 단어 수준 임베딩(2)
1. GloVe
 - 단어 간 유사도를 측정하기 어렵다는 잠재 의미 분석의 단점과 말뭉치 전체의 통계 정보는 반영되기 어렵다는 Word2Vec의 단점을 극복하고자 한 기법
 - 목적함수는 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장 빈도의 로그 값으로 정의됨. 각 단어의 벡터 사이의 내적 값과 두 단어 동시 등장 빈도의 로그 값의 차이가 최소화될수록 학습 손실이 작아짐
 - 말뭉치를 대상으로 단어-문맥 행렬을 만드는 것에서부터 학습을 시작한 후, 목적함수를 최소화하는 임베딩 벡터를 찾기 위한 행렬 분해를 수행

2. Swivel
 - 타깃 단어와 문맥 단어가 같이 자주 등장할수록 두 단어에 해당하는 벡터의 내적이 PMI값과 일치하도록 더 강제함
 - 특정 윈도우 내에서 타깃과 문맥 단어가 동시에 등장한 적이 한번도 없을 때는 별도의 목적함수 가 있음. 두 단어의 동시 등장 횟수를 0이 아닌 1로 가정하고 PMI값 계산
 - 예를 들어, 두 단어가 모두 고빈도 단어인데 동시 등장 빈도가 0이라면 의미상 무관한 단어일 것이라고 가정하지만 저빈도 단어인데 두 단어의 동시 등장 빈도가 없다면 의미상 관계가 일부 있을 수 있다고 봄


참고 자료: [도서] 한국어 임베딩, 딥러닝을 이용한 자연어 처리 입문   