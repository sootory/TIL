# 세미프로젝트
## 자연어처리(NLP) 이론 심화_2장
1. 자연어 계산과 이해
 - 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣기 때문에 자연어의 의미를 함축할 수 있음
 - 임베딩을 만들 때 활용하는 통계 정보는 1) 어떤 단어가 많이 쓰였는지, 2) 단어가 어떤 순서로 등장하는지, 3) 어떤 단어가 같이 나타났는지의 세 가지로 크게 나눌 수 있음

2. 어떤 단어가 많이 쓰였는가
 - Bag of words: 문장을 단어들로 나누고 그 빈도를 세어 놓은 것. 주제가 단어 사용에 녹아 있으며, 빈도가 높은 단어가 주제와 더 연관이 높을 것이라는 전제가 깔려 있음. 정보 검색 분야에서 많이 쓰임
 - TF-IDF: 위의 임베딩 기법 중 하나로 어떤 단어의 주제 예측 능력이 강할수록 가중치가 커지는 것. 조사와 같은 정보성이 낮은 단어들은 가중치가 줄어 불필요한 정보가 사라짐
 - Deep Averaging Network: Bag of words의 뉴럴 네트워크 버전으로 단어의 순서를 고려하지 않음. 문장 임베딩을 입력받아 해당 문서가 어떤 범주인지 분류함

3. 단어가 어떤 순서로 쓰였는가
- 통계 기반 언어 모델: 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습함. 하지만 데이터에 한번도 등장하지 않은 n-gram이 있을 경우엔 예측 단계에서 문제 발생 가능하기 때문에 이를 해소하기 위해 back-off, smoothing 등의 방식을 활용함
- 뉴럴 네트워크 기반 언어 모델: 주어진 단어 시퀀스를 통해 다음 단어를 맞추는 과정에서 학습됨. 학습이 완료되면 모델의 중간 혹은 마지막 계산 결과물을 임베딩으로 활용함. ELMo, GPT 등의 모델이 해당. 마스크 언어 모델은 문장 전체를 다 보고 중간 단어를 예측하기 때문에 양방향 학습이 가능함. BERT가 대표적인데 기존 언어 모델 기법들보다 임베딩 품질이 좋음

4. 어떤 단어가 같이 쓰였는가
 - 분포 가정: 어떤 단어 쌍이 비슷한 문맥에서 자주 등장한다면 의미가 유사할 것이라는 전제를 바탕으로 함. 타깃 단어들의 문맥 단어가 비슷하다면 타깃 단어 쌍도 유사할 가능성이 높다고 유추함
 - Word2Vec: 분포 가정의 대표적 모델로 기본 구조는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습하는 CBOW 모델과 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하면서 학습하는 Skip-gram 모델임. 유사한 단어를 추출하거나 산술을 통해 관계 있는 단어를 예측하는데 활용 가능함


참고 자료: [도서] 한국어 임베딩, 딥러닝을 이용한 자연어 처리 입문   
[블로그] [귀퉁이 서재_NLP - 11. 워드투벡터(Word2Vec)](https://bkshin.tistory.com/entry/NLP-11-Word2Vec)